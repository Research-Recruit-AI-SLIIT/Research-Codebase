{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Data/mindsetdsV2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "#remove the URLS\n",
    "data['sentence'] = data['sentence'].apply(lambda x: ' '.join([word for word in str(x).split() if word[0:4] not in ('http')]))\n",
    "#remove the words with @ and # from the text\n",
    "data['sentence'] = data['sentence'].apply(lambda x: ' '.join([word for word in str(x).split() if word[0] not in ('@', '#')]))\n",
    "data['sentence'] = data['sentence'].apply(lambda x: str(x).lower())\n",
    "data['sentence'] = data['sentence'].apply(lambda x: x.replace('\\n', ' '))\n",
    "data['sentence'] = data['sentence'].apply(lambda x: x.replace('\\t', ' '))\n",
    "data['sentence'] = data['sentence'].apply(lambda x: x.replace('\\r', ' '))\n",
    "data['sentence'] = data['sentence'].apply(lambda x: x.replace('\\xa0', ' '))\n",
    "data['sentence'] = data['sentence'].apply(lambda x: x.replace('\\u200b', ' '))\n",
    "data['sentence'] = data['sentence'].apply(lambda x: x.replace('\\ufeff', ' '))\n",
    "\n",
    "stop_words = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are',  'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'could',  'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadnt', 'has', 'have', 'havent', 'having', 'he', 'hed', 'hell', 'hes', 'her', 'here', 'heres', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'hows', 'i', 'id', 'ill', 'im', 'ive', 'if', 'in', 'into', 'is', 'isnt', 'it', 'its', 'its', 'itself', 'lets', 'me', 'more', 'most', 'mustnt', 'my', 'myself', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', 'she', 'shed', 'shell', 'shes', 'should', 'so', 'some', 'such', 'than', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'theres', 'these', 'they', 'theyd', 'theyll', 'theyre', 'theyve', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'wed', 'well', 'were', 'weve', 'were', 'what', 'whats', 'when', 'whens', 'where', 'wheres', \n",
    "        'which', 'while', 'who', 'whos', 'whom', 'why', 'whys', 'with', 'would', 'you', 'youd', 'youll', 'youre', 'youve', 'your', 'yours', 'yourself', 'yourselves']\n",
    "#removing stop words\n",
    "data['sentence'] = data['sentence'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "\n",
    "#remove punchations\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "data['sentence'] = data['sentence'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (punctuations)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "#if positive label = 2\n",
    "#if neutral label = 1\n",
    "#if negative label = 0\n",
    "\n",
    "data['label'] = data['label'].apply(lambda x: 2 if x == 'Positive' else x)\n",
    "data['label'] = data['label'].apply(lambda x: 1 if x == 'Neutral' else x)\n",
    "data['label'] = data['label'].apply(lambda x: 0 if x == 'Negative' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train, validation and test\n",
    "train_dataset = data.sample(frac=0.8, random_state=423)\n",
    "test_dataset = data.drop(train_dataset.index)\n",
    "validation_dataset = test_dataset.sample(frac=0.5, random_state=423)\n",
    "test_dataset = test_dataset.drop(validation_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_dataset['sentence'], train_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices((validation_dataset['sentence'], validation_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47831</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34002</th>\n",
       "      <td>0</td>\n",
       "      <td>collateral revolver satisfying youtu.be/_quo1_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>0</td>\n",
       "      <td>highest disrespect.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>1</td>\n",
       "      <td>business pulling wrong erp system?. upgrade mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57466</th>\n",
       "      <td>0</td>\n",
       "      <td>hey guys. tom clancy ’ s ghost recon wild land...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                           sentence\n",
       "47831      0                                                   \n",
       "34002      0  collateral revolver satisfying youtu.be/_quo1_...\n",
       "39988      0                                highest disrespect.\n",
       "29996      1  business pulling wrong erp system?. upgrade mi...\n",
       "57466      0  hey guys. tom clancy ’ s ghost recon wild land..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the first 5 rows of the train dataset\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 16\n",
    "seed = 43\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=len(data)).batch(batch_size)\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.shuffle(buffer_size=len(val_ds)).batch(batch_size)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_encoder = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "tfhub_prepocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mindset_evaluation_model():\n",
    "    text_input = keras.Input(shape=(), dtype='string', name='text_input')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_prepocess, name='Preprocessing_for_BERT')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_encoder, trainable=False, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    x = keras.layers.Dense(128, activation='sigmoid')(outputs['sequence_output'])\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(64, activation='sigmoid')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(32, activation='sigmoid')(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(3, activation='softmax')(x)\n",
    "    model = keras.Model(inputs=[text_input], outputs=x)\n",
    "    model.compile(optimizer=Adam(lr=0.0001),\n",
    "                    loss='sparse_categorical_crossentropy',             \n",
    "                    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " text_input (InputLayer)        [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " Preprocessing_for_BERT (KerasL  {'input_type_ids':   0          ['text_input[0][0]']             \n",
      " ayer)                          (None, 128),                                                      \n",
      "                                 'input_mask': (Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 'input_word_ids':                                                \n",
      "                                (None, 128)}                                                      \n",
      "                                                                                                  \n",
      " BERT_encoder (KerasLayer)      {'sequence_output':  109482241   ['Preprocessing_for_BERT[0][0]', \n",
      "                                 (None, 128, 768),                'Preprocessing_for_BERT[0][1]', \n",
      "                                 'pooled_output': (               'Preprocessing_for_BERT[0][2]'] \n",
      "                                None, 768),                                                       \n",
      "                                 'default': (None,                                                \n",
      "                                768),                                                             \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 128, 768),                                               \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768),                                                \n",
      "                                 (None, 128, 768)]}                                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128, 128)     98432       ['BERT_encoder[0][14]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128, 128)     0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128, 64)      8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128, 64)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128, 32)      2080        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128, 32)      0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4096)         0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3)            12291       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,603,300\n",
      "Trainable params: 121,059\n",
      "Non-trainable params: 109,482,241\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\research\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# create the deep learning model to classify text into three categories\n",
    "text_input = keras.Input(shape=(), dtype='string', name='text_input')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_prepocess, name='Preprocessing_for_BERT')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_encoder, trainable=False, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "x = keras.layers.Dense(128, activation='relu')(outputs['sequence_output'])\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.25)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(3, activation='softmax')(x)\n",
    "model = keras.Model(inputs=[text_input], outputs=x)\n",
    "model.compile(optimizer=Adam(lr=0.0001),\n",
    "                loss='sparse_categorical_crossentropy',             \n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "# logs about the training process\n",
    "# accuracy and loss are plotted in the TensorBoard\n",
    "log_dir = \"logs/fit/\" + str(int(time.time()))\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3085/3085 [==============================] - 710s 227ms/step - loss: 0.8778 - accuracy: 0.5966 - val_loss: 0.8018 - val_accuracy: 0.6507\n",
      "Epoch 2/50\n",
      "3085/3085 [==============================] - 888s 288ms/step - loss: 0.7766 - accuracy: 0.6614 - val_loss: 0.7609 - val_accuracy: 0.6700\n",
      "Epoch 3/50\n",
      " 600/3085 [====>.........................] - ETA: 12:39 - loss: 0.7426 - accuracy: 0.6811"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_ds,\n",
    "                    validation_data=val_ds,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "                        keras.callbacks.TensorBoard(\"logs/fit/\" + str(int(time.time())),\n",
    "                                                    histogram_freq=1,\n",
    "                                                    write_graph=True,\n",
    "                                                    write_images=True),\n",
    "                        keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                      monitor='val_loss')\n",
    "\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((test_dataset['sentence'], test_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "test_ds = test_ds.shuffle(buffer_size=len(test_ds)).batch(batch_size)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "results = model.evaluate(test_ds, verbose=1, callbacks=[\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs',\n",
    "                                histogram_freq=1,\n",
    "                                write_graph=True,\n",
    "                                write_images=True)]\n",
    ")\n",
    "\n",
    "\n",
    "# print the results\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw the accuracy and loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test Model with Sample Test Data\n",
    "text = 'Personally I am not go in that way. It is a stress for me. '\n",
    "\n",
    "\n",
    "# get prdiction from the model\n",
    "prediction = model.predict([text])\n",
    "\n",
    "#get the label from the prediction\n",
    "label = np.argmax(prediction)\n",
    "\n",
    "# print the label\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the test_dataset by half randomly\n",
    "t2dataset = test_dataset.sample(frac=1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# get the predictions\n",
    "predictions = model.predict(t2dataset['sentence'])\n",
    "\n",
    "# get the labels\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model name with date and time\n",
    "model_name = 'model_' + str(int(time.time()))\n",
    "\n",
    "# save the model\n",
    "model.save(model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_1665444162'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confusion matrix for three classes\n",
    "from operator import index\n",
    "from re import M\n",
    "\n",
    "\n",
    "cm = confusion_matrix(t2dataset['label'], y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# create directory\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(\"./Model Evaluation/\" + model_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#save confusion matrix\n",
    "plt.savefig('./Model Evaluation/' + model_name + '/confusion_matrix.png')\n",
    "\n",
    "#calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(t2dataset['label'], y_pred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "#f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(t2dataset['label'], y_pred, average='macro')\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "#precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(t2dataset['label'], y_pred, average='macro')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "#recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(t2dataset['label'], y_pred, average='macro')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "\n",
    "# save matrix into a txt file\n",
    "with open('./Model Evaluation/' + model_name + '/matrix.txt', 'w') as f:\n",
    "    f.write('Accuracy: %f \\n' % accuracy)\n",
    "    f.write('F1 score: %f \\n' % f1)\n",
    "    f.write('Precision: %f \\n' % precision)\n",
    "    f.write('Recall: %f \\n' % recall)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error Analysis\n",
    "\n",
    "#Root Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rms = sqrt(mean_squared_error(t2dataset['label'], y_pred))\n",
    "print('Root Mean Squared Error: %f' % rms)\n",
    "\n",
    "#Mean Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(t2dataset['label'], y_pred)\n",
    "\n",
    "print('Mean Absolute Error: %f' % mae)\n",
    "\n",
    "#Mean Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(t2dataset['label'], y_pred)\n",
    "\n",
    "print('Mean Squared Error: %f' % mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('./Models/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = keras.models.load_model('./Models/' + 'model_1665444162')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the model\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='./Model Output/' + model_name + '.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#save the plots\n",
    "plt.savefig('./Model Evaluation/' + model_name + '/accuracy.png')\n",
    "plt.savefig('./Model Evaluation/' + model_name + '/loss.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6dd968e44b48f26426fe23fbd93cd49fd04e6edb7e547023c0509ffaabbd7da7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
