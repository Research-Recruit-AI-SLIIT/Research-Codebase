{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages for BERT\n",
    "!pip install -q transformers\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow-hub\n",
    "!pip install -q bert-serving-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\research\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bert_similarity(base_answer, sample_answer):\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "    tokenize_base_answer = sent_tokenize(base_answer)\n",
    "    base_answer_embedding = model.encode(tokenize_base_answer)\n",
    "    base_answer_embedding_mean = np.mean(np.array(base_answer_embedding), axis=0)\n",
    "\n",
    "    tokenize_sample_answer = sent_tokenize(sample_answer)\n",
    "    sample_answer_embedding = model.encode(tokenize_sample_answer)\n",
    "    sample_answer_embedding_mean = np.mean(np.array(sample_answer_embedding), axis=0)\n",
    "\n",
    "    cosine_similarity_score = cosine_similarity([base_answer_embedding_mean], [sample_answer_embedding_mean]).flatten()\n",
    "\n",
    "    print(cosine_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8873448]\n"
     ]
    }
   ],
   "source": [
    "model_ans_p = \"Narendra Modi is the prime minister of India\"\n",
    "ans_p = \"Narendra Modi is the president of India\"\n",
    "process_bert_similarity(model_ans_p, ans_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9718205]\n"
     ]
    }
   ],
   "source": [
    "base_answer = \"President of America is Donald Trump. And president of India is Narendra Modi.\"\n",
    "sample_answer = \"Alendra Modi is the president of India. And Donald Trump is the president of America.\"\n",
    "process_bert_similarity(base_answer, sample_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyWordsExtraction(text):\n",
    "    model = KeyBERT()\n",
    "    key_words = model.extract_keywords(text)\n",
    "    return key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords1 = keyWordsExtraction(base_answer)\n",
    "keyWords2 = keyWordsExtraction(sample_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words1 = []\n",
    "key_words2 = []\n",
    "\n",
    "for key_word in keyWords1:\n",
    "    key_words1.append(key_word[0])\n",
    "\n",
    "for key_word in keyWords2:\n",
    "    key_words2.append(key_word[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['president', 'trump', 'narendra', 'india', 'donald']\n",
      "['modi', 'india', 'president', 'alendra', 'trump']\n"
     ]
    }
   ],
   "source": [
    "print(key_words1)\n",
    "print(key_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textpreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n",
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "key_words1 = textpreprocessing.preprocess_answer(\n",
    "    \"hello\"\n",
    ")\n",
    "print(key_words1)\n",
    "print(key_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9820568]\n"
     ]
    }
   ],
   "source": [
    "key_words1 = []\n",
    "key_words2 = []\n",
    "\n",
    "for key_word in keyWords1:\n",
    "    key_words1.append(key_word[0])\n",
    "\n",
    "for key_word in keyWords2:\n",
    "    key_words2.append(key_word[0])\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "base_answer_embedding = model.encode(key_words1)\n",
    "base_answer_embedding_mean = np.mean(np.array(base_answer_embedding), axis=0)\n",
    "\n",
    "sample_answer_embedding = model.encode(key_words2)\n",
    "sample_answer_embedding_mean = np.mean(np.array(sample_answer_embedding), axis=0)\n",
    "\n",
    "cosine_similarity_score = cosine_similarity([base_answer_embedding_mean], [sample_answer_embedding_mean]).flatten()\n",
    "print(cosine_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogInfo(stri):\n",
    "    print(str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))+'  '+stri)\n",
    "    \n",
    "def preprocess_data_en(stopwords,doc):\n",
    "    '''\n",
    "    Function: preprocess data in Chinese including cleaning, tokenzing...\n",
    "    Input: document string\n",
    "    Output: list of words\n",
    "    '''     \n",
    "    doc = doc.lower()\n",
    "    doc = word_tokenize(doc)\n",
    "    doc = [word for word in doc if word not in set(stopwords)]\n",
    "    doc = [word for word in doc if word.isalpha()]\n",
    "    return doc\n",
    "\n",
    "def preprocess_data_cn(stopwords,doc):\n",
    "    '''\n",
    "    Function: preprocess data in Chinese including cleaning, tokenzing...\n",
    "    Input: \n",
    "        stopwords: Chinese stopwords list\n",
    "        doc: document string\n",
    "    Output: list of words\n",
    "    '''       \n",
    "    # clean data\n",
    "    doc = re.sub(u\"[^\\u4E00-\\u9FFF]\", \"\", doc) # delete all non-chinese characters\n",
    "    doc = re.sub(u\"[儿]\", \"\", doc) # delete 儿\n",
    "    # tokenize and move stopwords \n",
    "#     doc = [word for word in jieba.cut(doc) if word not in set(stopwords)]  \n",
    "    words = []\n",
    "    pos = ['zg','e','y','o','ul','ud','uj','z'] # 定义需要过滤的词性\n",
    "    # zg:哦 e:嗯 y:啦 o:哈哈 ul:了 r:他，你，哪儿，哪里 ug:过 z:咋啦\n",
    "    seg = jieba.posseg.cut(doc)  # 分词\n",
    "    for i in seg:   \n",
    "        if i.flag not in pos and i.word not in stopwords :  # 去停用词 + 词性筛选\n",
    "            words.append(i.word)            \n",
    "    return words\n",
    "\n",
    "def filter_words(vocab,doc):\n",
    "    '''\n",
    "    Function: filter words which are not contained in the vocab\n",
    "    Input:\n",
    "        vocab: list of words that have word2vec representation\n",
    "        doc: list of words in a document\n",
    "    Output:\n",
    "        list of filtered words\n",
    "    '''\n",
    "    return [word for word in doc if word in vocab]\n",
    "\n",
    "def f(x):\n",
    "    if x<0.0: return 0.0\n",
    "    else: return x\n",
    "    \n",
    "def handle_sim(x):  \n",
    "    return 1.0-np.vectorize(f)(x)\n",
    "\n",
    "def regularize_sim(sims):\n",
    "    '''\n",
    "    Function: replace illegal similarity value -1 with mean value\n",
    "    Input: list of similarity of document pairs\n",
    "    Output: regularized list of similarity \n",
    "    '''\n",
    "    sim_mean = np.mean([sim for sim in sims if sim!=-1])\n",
    "    r_sims = []\n",
    "    errors = 0\n",
    "    for sim in sims:\n",
    "        if sim==-1:\n",
    "            r_sims.append(sim_mean)\n",
    "            errors += 1\n",
    "        else:\n",
    "            r_sims.append(sim)\n",
    "#     LogInfo('Regularize: '+str(errors))\n",
    "    return r_sims\n",
    "\n",
    "def load_word2vec(model_path):\n",
    "    model = dict()\n",
    "    for line in open(model_path,encoding='utf-8'):\n",
    "        l = line.strip().split()    \n",
    "        st=' '.join(l[:-300]).lower()   \n",
    "        model[st]=list(map(float,l[-300:]))\n",
    "  \n",
    "    num_keys=len(model)\n",
    "   \n",
    "    return model\n",
    "\n",
    "\n",
    "def wmd_sim(lang,docs1,docs2):\n",
    "    '''\n",
    "    Function:\n",
    "        calculate similarity of document pairs \n",
    "    Input: \n",
    "        lang: text language-Chinese for 'cn'/ English for 'en'\n",
    "        docs1:  document strings list1\n",
    "        docs2: document strings list2\n",
    "    Output:\n",
    "        similarity list of docs1 and docs2 pairs: value ranges from 0 to 1; \n",
    "                  \n",
    "    '''\n",
    "    # check if the number of documents matched\n",
    "    assert len(docs1)==len(docs2) ,'Documents number is not matched!'\n",
    "    assert len(docs1)!=0,'Documents list1 is null'\n",
    "    assert len(docs2)!=0,'Documents list2 is null'\n",
    "    assert lang=='cn' or lang=='en', 'Language setting is wrong'\n",
    "    # change setting according to text language \n",
    "    if lang=='cn':\n",
    "        model_path = '../model/cn.cbow.bin'\n",
    "        stopwords_path = 'chinese_stopwords.txt'\n",
    "        preprocess_data = preprocess_data_cn\n",
    "    elif lang=='en':\n",
    "        model_path = '../model/GoogleNews-vectors-negative300.bin'\n",
    "        stopwords_path = 'english_stopwords.txt'\n",
    "        preprocess_data = preprocess_data_en\n",
    "    # load word2vec model  \n",
    "    LogInfo('Load word2vec model...')\n",
    "#     model = load_word2vec('../model/sgns.baidubaike.bigram-char')\n",
    "#     vocab = list(model.keys())\n",
    "    model = KeyedVectors.load_word2vec_format(model_path,binary=True,unicode_errors='ignore')\n",
    "    vocab = model.vocab\n",
    "\n",
    "    # preprocess data\n",
    "    stopwords= set(w.strip() for w in codecs.open(stopwords_path, 'r',encoding='utf-8').readlines())\n",
    "    sims = []\n",
    "    LogInfo('Calculating similarity...')\n",
    "    for i in range(len(docs1)):        \n",
    "        p1 = preprocess_data(stopwords,docs1[i])\n",
    "        p2 = preprocess_data(stopwords,docs2[i])\n",
    "        p1 = filter_words(vocab,p1)\n",
    "        p2 = filter_words(vocab,p2)\n",
    "        if len(p1)==0 or len(p2)==0:\n",
    "            # if any filtered document is null, return -1 \n",
    "            sim = -1\n",
    "        else:\n",
    "            p1 = ' '.join(p1)\n",
    "            p2 = ' '.join(p2)\n",
    "            vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b', stop_words=None)\n",
    "            v1,v2 = vectorizer.fit_transform([p1,p2])\n",
    "            # pyemd needs double precision input\n",
    "            v1 = v1.toarray().ravel().astype(np.double)\n",
    "            v2 = v2.toarray().ravel().astype(np.double)\n",
    "            # transform word count to frequency [0,1]\n",
    "            v1 /= v1.sum()\n",
    "            v2 /= v2.sum()\n",
    "            # obtain word2vec representations \n",
    "            W = [model[word] for word in vectorizer.get_feature_names()]\n",
    "            # calculate distance matrix (distance = 1-cosine similarity) [0,1]\n",
    "            D = handle_sim(cosine_similarity(W)).astype(np.double)         \n",
    "            # calculate minimal distance using EMD algorithm\n",
    "            min_distance = emd(v1,v2,D)\n",
    "            # calculate similarity (similarity = 1-min_distance)\n",
    "            sim = 1-min_distance\n",
    "        \n",
    "        sims.append(sim)\n",
    "    # regularize similarity: replace -1 with average similarity\n",
    "    rsims = regularize_sim(sims) \n",
    "    # 只保留小数点后四位\n",
    "    rsims = [round(sim,4) for sim in rsims]\n",
    "    return rsims\n",
    "\n",
    "def compute_ser(sims):\n",
    "    '''\n",
    "    Function: compute SER(semantic error rate) according to the document similarity\n",
    "    Input: \n",
    "        sims: list of document similarity\n",
    "    Output:\n",
    "        sers: list of document SER\n",
    "    '''\n",
    "    sers = [round(1.0-sim,4) for sim in sims]\n",
    "    return sers\n",
    "\n",
    "def example():\n",
    "    # English text example\n",
    "    docs1 = ['man sitting using tool at a table in his home.',\n",
    "                 'vegetable is being sliced.',\n",
    "                'a speaker presents some products']\n",
    "    docs2 = ['The president comes to China',\n",
    "                'someone is slicing a tomato with a knife on a cutting board.',\n",
    "                'the speaker is introducing the new products on a fair.']\n",
    "    # calculate similarity\n",
    "    sims = wmd_sim('en',docs1,docs2)\n",
    "    # calculate SER\n",
    "    sers = compute_ser(sims)\n",
    "    # print result\n",
    "    for i in range(len(sims)):\n",
    "        print(docs1[i])\n",
    "        print(docs2[i])\n",
    "        print('Similarity: %.4f' %sims[i])\n",
    "        print('SER: %.4f' %sers[i])\n",
    "        \n",
    "    # Chinese text example\n",
    "    docs1 = ['时间太晚不得就算了', \n",
    "            '他整天愁眉苦脸',\n",
    "             '学无止境'] \n",
    "             \n",
    "    docs2 = ['此间贷款不得就算啦', \n",
    "            '他和朋友去逛街',\n",
    "             '学海无涯，天道酬勤']\n",
    "    # calculate similarity\n",
    "    sims = wmd_sim('cn',docs1,docs2)\n",
    "    # calculate SER\n",
    "    sers = compute_ser(sims)\n",
    "    # print result\n",
    "    for i in range(len(sims)):\n",
    "        print(docs1[i])\n",
    "        print(docs2[i])\n",
    "        print('Similarity: %.4f' %sims[i])\n",
    "        print('SER: %.4f' %sers[i])\n",
    "        \n",
    "def main_cn():\n",
    "    corpus = ['baidu_003_02','weixin_003_02','ifly_003_02',\n",
    "              'baidu_008','weixin_008','ifly_008',\n",
    "                'baidu_004','weixin_004', 'ifly_004',\n",
    "                'baidu_006_01','weixin_006_01', 'ifly_006_01',           \n",
    "                'baidu_004_02','weixin_004_02','ifly_004_02',\n",
    "               'baidu_rePunct_huiting','weixin_rePunct_huiting', 'ifly_rePunct_huiting']\n",
    "\n",
    "    for c in corpus:\n",
    "        LogInfo(c+' start')     \n",
    "        # read data\n",
    "        data = pd.read_csv('../data/'+c+'.csv')\n",
    "        docs1 = data.REF.values\n",
    "        docs2 = data.HYP.values\n",
    "        # calculate similarity\n",
    "        sims = wmd_sim('cn',docs1,docs2)\n",
    "        # calculate SER\n",
    "        sers = compute_ser(sims)\n",
    "        # save result as .xls\n",
    "        save_path = '../../wechat_semantic_similarity/res/'+c+'_wmd2.xls'\n",
    "        res = pd.DataFrame(columns=['id','REF','HYP','semantic_similarity','SER','WER','difference'])\n",
    "        res.id = data.id\n",
    "        res.REF = docs1\n",
    "        res.HYP = docs2\n",
    "        res.WER = data.WER\n",
    "        res.semantic_similarity = sims  \n",
    "        res.SER = sers\n",
    "        res.difference = res.SER-res.WER\n",
    "        res.to_excel(save_path,index=0)\n",
    "        LogInfo(c+' finish')\n",
    "\n",
    "def main_en():\n",
    "    LogInfo('Start')\n",
    "    path1 = '../data/train_data1.txt'\n",
    "    path2 = '../data/train_data2.txt'\n",
    "    data1 = codecs.open(path1,'r',encoding='utf-8').read().split('\\r\\n')[:-1]\n",
    "    data2 = codecs.open(path2,'r',encoding='utf-8').read().split('\\r\\n')[:-1]\n",
    "    # calculate similarity\n",
    "    sims = wmd_sim('en',data1,data2)\n",
    "    # calculate SER\n",
    "    sers = compute_ser(sims)\n",
    "    # save result as .xls\n",
    "    save_path = '../res/res_english_.csv'\n",
    "    res = pd.DataFrame(columns=['REF','HYP','semantic_similarity','SER'])\n",
    "    res.REF = data1\n",
    "    res.HYP = data2\n",
    "    res.semantic_similarity = sims\n",
    "    res.SER = sers\n",
    "    res.to_csv(save_path,index=0)\n",
    "    LogInfo('Save result as: '+save_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example()\n",
    "#     main_cn()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42588fd11209419b7c81abe23e31a1d2101f60811b02170c268e57ebb57d5d9a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
